# Configuration for AutoCoRe-FL Stages 1&2 followed by LR-XFL Stage 3
method_name: "LRXFL_FL_AutoCore_sun"
seed: 42
device: "cuda" 
run_id_base: "LRXFL_FL_AutoCore_sun" 
log_level: "INFO"

num_classes: 3 # Number of classes in the dataset (for logging and checks)
# --- Chosen Classes (Documentation, num_classes derived from loaded map) ---
chosen_classes:
  - "bathroom"
  - "bedroom"
  - "bookstore"

# --- Cached Data Input Paths ---
cached_data_base_dir: "./generated_autocore_fl_caches_reused_npy_10clients_sun/sun_reused_npy" # !!! ADJUST AS DESIRED !!!
partition_segment_infos_cache_dir_input: "partition_segment_infos_with_crops"
embedding_cache_dir_input: "partition_embedding_cache"
partition_manifest_dir_input: "partition_manifests"
scene_to_idx_map_filename_input: "scene_to_idx_map.json"
noise_experiment_enabled: false     # or false
noise_client_percentage: 0.2      # e.g., 20% of clients
noise_label_shuffle_degree: 0.5  # e.g., 50% of their labels shuffled
# --- AutoCoRe-FL: SAM/DINO Models (for internal client/server use in Stage 2) ---
sam_cfg: "configs/sam2.1/sam2.1_hiera_t.yaml"
sam_ckpt: "/gpfs/helios/home/soliman/logic_explained_networks/experiments/sam2.1_hiera_tiny.pt" # !!! ADJUST !!!
dino_model: "facebook/dinov2-base"

# --- AutoCoRe-FL: Embedding Settings (Must match cached embeddings) ---
embedding_type: "dino_only"
embedding_dim: 768

# --- AutoCoRe-FL: General FL Parameters ---
num_clients: 10 # Must match  cached data

# --- AutoCoRe-FL: Stage 1 K-Means Parameters ---
num_clusters: 35
kmeans_rounds: 10
min_samples_per_concept_cluster: 20

# --- AutoCoRe-FL: Stage 2 Concept Detector Parameters ---
detector_type: "lr"
detector_min_samples_per_class: 20
detector_cv_splits: 3
pca_n_components: 128
lr_max_iter: 10000
svm_C: 1.0
min_detector_score: 0.65
vectorizer_min_activating_segments: 1

# --- LR-XFL: Stage 3 Federated Rule Learning Parameters ---
# These are from your 3_run_xfl_with_predicted_concepts.py
lrxfl_max_epoch: 10                # Global FL rounds for LR-XFL
lrxfl_local_epochs: 30             # Local epochs for client's Explainer training
lrxfl_sample_type: 'non-iid'           # Data sampling for LR-XFL clients (will be effectively IID due to AutoCoRe partitioning)
lrxfl_topk_explanations_local: 50
lrxfl_topk_explanations_global: 3  # For server aggregation in LR-XFL
lrxfl_logic_generation_threshold: 0.8 # Min accuracy for local Explainer to generate rules
lrxfl_learning_rate: 0.01          # LR for the Explainer model
lrxfl_l1_reg: 0.001               # L1 regularization for Explainer
lrxfl_temperature: 0.7             # Temperature for EntropyLinear layer in Explainer
lrxfl_explainer_hidden: [10]       # Hidden layers for the Explainer MLP
lrxfl_batch_size: 64               # Batch size for LR-XFL client DataLoaders

# --- Output Directory for this Combined Run ---
output_base_dir_combined_run: "./results/lrxfl_fl_autocore_results_sun" # !!! ADJUST AS DESIRED !!!